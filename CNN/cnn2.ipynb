{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # kernel size = set the filter size \n",
    "        # in_channels = Depend on the number of color channels of the input\n",
    "        # out_channels = Set the number of filters \n",
    "        # increase the number of channels during convolution \n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "        \n",
    "        # shrink the channels during the linear function\n",
    "        # 12*4*4 means you have flatten the matrix\n",
    "            # 12 = out_channel\n",
    "            # 4 * 4 = \n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        # (1) input layer \n",
    "        t = t \n",
    "        # output: [1,1,28,28]\n",
    "        \n",
    "        # (2) hidden conv layer \n",
    "        t = self.conv1(t)\n",
    "        # output: [1,6,24,24]\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        # output: [1,6,12,12]\n",
    "        \n",
    "        # (3) hidden conv layer \n",
    "        t = self.conv2(t)\n",
    "        # output: [1,12,8,8]\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        # output: [1,12,4,4]\n",
    "        \n",
    "        # (4) hidden linear layer \n",
    "        t = t.reshape(-1, 12 * 4 * 4)\n",
    "        # output: [1,192]\n",
    "        t = self.fc1(t)\n",
    "        # output: [1,120]\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (5) hidden linear layer \n",
    "        t = self.fc2(t)\n",
    "        # output: [1,60]\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        # (6) ouptut layer \n",
    "        t = self.out(t)\n",
    "        # output: [1,10]\n",
    "        # t = F.softmax(t, dim=1)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=192, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=60, bias=True)\n",
      "  (out): Linear(in_features=60, out_features=10, bias=True)\n",
      ")\n",
      "Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "Linear(in_features=192, out_features=120, bias=True)\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "print(network)\n",
    "print(network.conv1)\n",
    "print(network.conv2)\n",
    "print(network.fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.1711,  0.1359,  0.1461, -0.1989,  0.1453],\n",
       "          [-0.0698, -0.0657, -0.0920, -0.1884,  0.0459],\n",
       "          [ 0.0708, -0.1693,  0.0482, -0.1047, -0.0068],\n",
       "          [-0.1736,  0.0746, -0.1650,  0.1818,  0.0669],\n",
       "          [ 0.1697, -0.1069,  0.1543,  0.0256,  0.1557]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0585, -0.0941,  0.1178, -0.0015, -0.0087],\n",
       "          [-0.0934, -0.0563, -0.1759, -0.0917, -0.0847],\n",
       "          [ 0.1211,  0.0757, -0.0124, -0.0692, -0.1519],\n",
       "          [-0.0575, -0.0517,  0.0436,  0.0808,  0.0487],\n",
       "          [-0.1061, -0.1797,  0.1358, -0.0204, -0.1514]]],\n",
       "\n",
       "\n",
       "        [[[-0.1123, -0.1055, -0.0145, -0.1858, -0.0529],\n",
       "          [ 0.0909,  0.0193,  0.0174, -0.0206, -0.0836],\n",
       "          [-0.0279, -0.0998, -0.0645, -0.0419, -0.1884],\n",
       "          [ 0.0992,  0.0034,  0.0927, -0.1696,  0.1205],\n",
       "          [ 0.0170, -0.0437, -0.0316,  0.0492, -0.1920]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0059, -0.0579, -0.1822,  0.1153,  0.1392],\n",
       "          [ 0.0482,  0.1152, -0.0611, -0.1372, -0.1727],\n",
       "          [ 0.0250,  0.0429,  0.0788,  0.1292,  0.1788],\n",
       "          [ 0.0380, -0.0912,  0.0549, -0.0268,  0.1812],\n",
       "          [-0.1355, -0.1384,  0.0832,  0.0064,  0.1427]]],\n",
       "\n",
       "\n",
       "        [[[-0.0529,  0.1644, -0.0882, -0.1547, -0.1027],\n",
       "          [-0.0710, -0.1039,  0.0782,  0.1662,  0.0815],\n",
       "          [-0.1864, -0.1513, -0.1935, -0.0394, -0.0596],\n",
       "          [ 0.1927,  0.0174,  0.1833, -0.0582,  0.0417],\n",
       "          [ 0.1940,  0.0650, -0.1745, -0.0511, -0.0007]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0214,  0.0978, -0.0325,  0.1180, -0.0943],\n",
       "          [-0.1679, -0.0565,  0.1476, -0.1612,  0.0017],\n",
       "          [ 0.0652, -0.1911, -0.1651, -0.1053,  0.0058],\n",
       "          [-0.0215, -0.0860,  0.1323,  0.0997, -0.0815],\n",
       "          [-0.1527,  0.0876, -0.0622,  0.1186,  0.0633]]]], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 5, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.conv1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perform matrix multiplication to create a linear result \n",
    "in_features = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "weight_matrix = torch.tensor([ \n",
    "    [1,2,3,4],\n",
    "    [2,3,4,5],\n",
    "    [3,4,5,6]\n",
    "], dtype=torch.float32)\n",
    "weight_matrix.matmul(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight \t torch.Size([6, 1, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([12, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([12])\n",
      "fc1.weight \t torch.Size([120, 192])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([60, 120])\n",
      "fc2.bias \t torch.Size([60])\n",
      "out.weight \t torch.Size([10, 60])\n",
      "out.bias \t torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in network.named_parameters():\n",
    "    print(name, '\\t', param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = nn.Linear(in_features=4, out_features=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc.weight = nn.Parameter(weight_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30., 40., 50.], grad_fn=<SqueezeBackward3>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_printoptions(linewidth=120)\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '/Users/sunghohong/Desktop/data/FashionMNIST',\n",
    "    train=True, # data for the training set\n",
    "    download = True,\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor() # transform the data into tensors\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "network = Network()\n",
    "sample = next(iter(train_set))\n",
    "\n",
    "image, label = sample \n",
    "image.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = network(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0105,  0.0121, -0.0526,  0.0408,  0.0733,  0.0254, -0.0562,  0.0941, -0.0473,  0.1409]])\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 28, 28])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 10)\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = network(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0105,  0.0121, -0.0526,  0.0408,  0.0733,  0.0254, -0.0562,  0.0941, -0.0473,  0.1409],\n",
       "        [-0.0126,  0.0144, -0.0529,  0.0408,  0.0761,  0.0329, -0.0581,  0.0945, -0.0523,  0.1397],\n",
       "        [-0.0088,  0.0039, -0.0528,  0.0395,  0.0717,  0.0299, -0.0535,  0.1054, -0.0464,  0.1411],\n",
       "        [-0.0114,  0.0097, -0.0521,  0.0398,  0.0732,  0.0310, -0.0557,  0.0998, -0.0478,  0.1384],\n",
       "        [-0.0161,  0.0151, -0.0478,  0.0423,  0.0730,  0.0307, -0.0496,  0.0914, -0.0403,  0.1411],\n",
       "        [-0.0159,  0.0146, -0.0498,  0.0415,  0.0793,  0.0311, -0.0587,  0.0900, -0.0515,  0.1397],\n",
       "        [-0.0099,  0.0111, -0.0518,  0.0402,  0.0742,  0.0274, -0.0561,  0.0964, -0.0487,  0.1390],\n",
       "        [-0.0117,  0.0116, -0.0491,  0.0432,  0.0775,  0.0274, -0.0573,  0.0950, -0.0519,  0.1335],\n",
       "        [-0.0107,  0.0009, -0.0543,  0.0380,  0.0744,  0.0275, -0.0557,  0.1057, -0.0456,  0.1363],\n",
       "        [-0.0142,  0.0087, -0.0476,  0.0340,  0.0775,  0.0225, -0.0583,  0.0977, -0.0527,  0.1403]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 0, 0, 3, 0, 2, 7, 2, 5, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.argmax(dim=1).eq(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x13060e7f0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "torch.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.300213575363159"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch \n",
    "\n",
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "print(network.conv1.weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.300213575363159"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2714192867279053"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = network(images)\n",
    "loss = F.cross_entropy(preds, labels)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss 336.615659981966\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size = 100)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.01)\n",
    "\n",
    "total_loss = 0 \n",
    "\n",
    "for batch in train_loader:\n",
    "    images, labels = batch \n",
    "    \n",
    "    preds = network(images)\n",
    "    loss = F.cross_entropy(preds, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_loss += loss.item()\n",
    "\n",
    "print('epoch:',0, \"loss\", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set))\n",
    "print(len(train_set.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    for batch in loader:\n",
    "        images, labels = batch \n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat((all_preds, preds),dim=0)\n",
    "    return all_preds\n",
    "\n",
    "\n",
    "def get_num_correct()\n",
    "\n",
    "prediction_loader = torch.utils.data.DataLoader(train_set, batch_size=10000)\n",
    "train_preds = get_all_preds(network, prediction_loader)\n",
    "print(train_preds.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
